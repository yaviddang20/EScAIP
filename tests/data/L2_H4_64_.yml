model:
  name: src.EfficientlyScaledAttentionInteratomicPotential

  # Global Configs
  activation: gelu
  direct_force: true
  hidden_size: 64
  regress_forces: true
  use_fp16_backbone: false
  use_triton: false
  use_rotation_force: false
  batch_size: 3

  # Molecular Graph Configs
  avg_degree: 30
  avg_num_nodes: 73.0
  max_num_nodes_per_batch: 150
  enforce_max_neighbors_strictly: true
  distance_function: gaussian
  max_neighbors: 30
  max_num_elements: 90
  max_radius: 5.0
  otf_graph: true
  use_pbc: true
  use_pbc_single: false

  # Graph Neural Networks Configs
  atom_embedding_size: 65
  # atten_name: xformers
  atten_name: memory_efficient
  use_block_diag_mask: True
  atten_num_heads: 4
  edge_distance_embedding_size: 512
  edge_distance_expansion_size: 600
  node_direction_embedding_size: 64
  node_direction_expansion_size: 10
  num_layers: 2
  output_hidden_layer_multiplier: 2
  readout_hidden_layer_multiplier: 2

  # Feed Forward Network Configs
  ffn_bias: true
  ffn_hidden_layer_multiplier: 2
  ffn_name: MLP

  # Regularization Configs
  atten_dropout: 0.1
  dropout_edge: 0.1 # TBA
  mlp_dropout: 0.1
  normalization: rmsnorm
  residual_dropout: 0.1
  residual_norm_style: post
  stochastic_depth_prob: 0.05
  weight_decay: 0.01
